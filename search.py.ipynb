{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "6147f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from lxml import etree\n",
    "import time\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    start = time.time()\n",
    "    with gzip.open('data/enwiki-latest-abstract18.xml.gz', 'rb') as f:\n",
    "        doc_id = 0\n",
    "        for _, element in etree.iterparse(f, events=('end',), tag='doc'):\n",
    "            title = element.findtext('./title')\n",
    "            url = element.findtext('./url')\n",
    "            abstract = element.findtext('./abstract')\n",
    "\n",
    "            yield Abstract(ID=doc_id, title=title, url=url, abstract=abstract)\n",
    "\n",
    "            doc_id += 1\n",
    "            element.clear()\n",
    "    end = time.time()\n",
    "    print(f'Parsing XML took {end - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "ac89388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyStemmer\n",
    "#ANALYSIS \n",
    "import re\n",
    "import string\n",
    "import Stemmer\n",
    "\n",
    "# top 25 most common words in English and \"wikipedia\":\n",
    "# https://en.wikipedia.org/wiki/Most_common_words_in_English\n",
    "STOPWORDS = set(['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',\n",
    "                 'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',\n",
    "                 'do', 'at', 'this', 'but', 'his', 'by', 'from', 'wikipedia'])\n",
    "PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "STEMMER = Stemmer.Stemmer('english')\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def lowercase_filter(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def punctuation_filter(tokens):\n",
    "    return [PUNCTUATION.sub('', token) for token in tokens]\n",
    "\n",
    "def stopword_filter(tokens):\n",
    "    return [token for token in tokens if token not in STOPWORDS]\n",
    "\n",
    "def stem_filter(tokens):\n",
    "    return STEMMER.stemWords(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "56639ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = lowercase_filter(tokens)\n",
    "    tokens = punctuation_filter(tokens)\n",
    "    tokens = stopword_filter(tokens)\n",
    "    tokens = stem_filter(tokens)\n",
    "\n",
    "    return [token for token in tokens if token]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8b9b7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timing(method):\n",
    "    \"\"\"\n",
    "    Quick and dirty decorator to time functions: it will record the time when\n",
    "    it's calling a function, record the time when it returns and compute the\n",
    "    difference. There'll be some overhead, so it's not very precise, but'll\n",
    "    suffice to illustrate the examples in the accompanying blog post.\n",
    "    @timing\n",
    "    def snore():\n",
    "        print('zzzzz')\n",
    "        time.sleep(5)\n",
    "    snore()\n",
    "    zzzzz\n",
    "    snore took 5.0011749267578125 seconds\n",
    "    \"\"\"\n",
    "    def timed(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = method(*args, **kwargs)\n",
    "        end = time.time()\n",
    "\n",
    "        execution_time = end - start\n",
    "        if execution_time < 0.001:\n",
    "            print(f'{method.__name__} took {execution_time*1000} milliseconds')\n",
    "        else:\n",
    "            print(f'{method.__name__} took {execution_time} seconds')\n",
    "\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "492c0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Index:\n",
    "    def __init__(self):\n",
    "        self.index = {}\n",
    "        self.documents = {}\n",
    "\n",
    "    def index_document(self, document):\n",
    "        if document.ID not in self.documents:\n",
    "            self.documents[document.ID] = document\n",
    "            document.analyze()\n",
    "\n",
    "        for token in analyze(document.fulltext):\n",
    "            if token not in self.index:\n",
    "                self.index[token] = set()\n",
    "            self.index[token].add(document.ID)\n",
    "\n",
    "    def document_frequency(self, token):\n",
    "        return len(self.index.get(token, set()))\n",
    "\n",
    "    def inverse_document_frequency(self, token):\n",
    "        # Manning, Hinrich and SchÃ¼tze use log10, so we do too, even though it\n",
    "        # doesn't really matter which log we use anyway\n",
    "        # https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html\n",
    "        return math.log10(len(self.documents) / self.document_frequency(token))\n",
    "\n",
    "    def _results(self, analyzed_query):\n",
    "        return [self.index.get(token, set()) for token in analyzed_query]\n",
    "\n",
    "\n",
    "    def rank(self, analyzed_query, documents):\n",
    "        results = []\n",
    "        if not documents:\n",
    "            return results\n",
    "        for document in documents:\n",
    "            score = 0.0\n",
    "            for token in analyzed_query:\n",
    "                tf = document.term_frequency(token)\n",
    "                idf = self.inverse_document_frequency(token)\n",
    "                score += tf * idf\n",
    "            results.append((document, score))\n",
    "        return sorted(results, key=lambda doc: doc[1], reverse=True)\n",
    "\n",
    "    @timing\n",
    "    def search(self, query, search_type='AND', rank=True):\n",
    "        \"\"\"\n",
    "        Still boolean search; this will return documents that contain either all words\n",
    "        from the query or just one of them, depending on the search_type specified.\n",
    "\n",
    "        We are still not ranking the results (sets are fast, but unordered).\n",
    "        \"\"\"\n",
    "        if search_type not in ('AND', 'OR'):\n",
    "            return []\n",
    "        analyzed_query = analyze(query)\n",
    "        \n",
    "        print(analyzed_query)\n",
    "        results = self._results(analyzed_query)\n",
    "        \n",
    "\n",
    "        if search_type == 'AND':\n",
    "          # all tokens must be in the document\n",
    "          documents = [self.documents[doc_id] for doc_id in set.intersection(*results)]\n",
    "        if search_type == 'OR':\n",
    "            \n",
    "          # only one token has to be in the document\n",
    "            documents = [self.documents[doc_id] for doc_id in set.union(*results)]\n",
    "        \n",
    "        if rank:\n",
    "            return self.rank(analyzed_query, documents)\n",
    "        return documents\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c608bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "@dataclass\n",
    "class Abstract:\n",
    "    \"\"\"Wikipedia abstract\"\"\"\n",
    "    ID: int\n",
    "    title: str\n",
    "    abstract: str\n",
    "    url: str\n",
    "    term_frequencies = {}\n",
    "    \n",
    "    @property\n",
    "    def fulltext(self):\n",
    "        return ' '.join([self.title, self.abstract])\n",
    "\n",
    "    def analyze(self):\n",
    "        # Counter will create a dictionary counting the unique values in an array:\n",
    "        # {'london': 12, 'beer': 3, ...}\n",
    "        self.term_frequencies = Counter(analyze(self.fulltext))\n",
    "\n",
    "    def term_frequency(self, term):\n",
    "        return self.term_frequencies.get(term, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "9af5c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNNER\n",
    "import os.path\n",
    "import requests\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_wikipedia_abstracts():\n",
    "    URL = 'https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-abstract18.xml.gz'\n",
    "    with requests.get(URL, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open('data/enwiki-latest-abstract18.xml.gz', 'wb') as f:\n",
    "            # write every 1mb\n",
    "            for i, chunk in enumerate(r.iter_content(chunk_size=1024*1024)):\n",
    "                f.write(chunk)\n",
    "                if i % 10 == 0:\n",
    "                    print(f'Downloaded {i} megabytes', end='\\r')\n",
    "\n",
    "\n",
    "@timing\n",
    "def index_documents(documents, index):\n",
    "    for i, document in enumerate(documents):\n",
    "        index.index_document(document)\n",
    "        if i % 5000 == 0:\n",
    "            print(f'Indexed {i} documents', end='\\r')\n",
    "        if i == 20000:\n",
    "            break\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6e527152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_documents took 1.498734712600708 seconds\n",
      "Index contains 20001 documents\n",
      "Intersection\n",
      "['laxmi', 'prasad']\n",
      "search took 0.07510185241699219 milliseconds\n",
      "[(Abstract(ID=6, title='Wikipedia: Laxmi Prasad SC', abstract='| ground        = Duler Stadium, Goa', url='https://en.wikipedia.org/wiki/Laxmi_Prasad_SC'), 8.00004342836249)]\n",
      "\n",
      "Union\n",
      "['laxmi', 'prasad']\n",
      "search took 0.028133392333984375 milliseconds\n",
      "[(Abstract(ID=6, title='Wikipedia: Laxmi Prasad SC', abstract='| ground        = Duler Stadium, Goa', url='https://en.wikipedia.org/wiki/Laxmi_Prasad_SC'), 8.00004342836249), (Abstract(ID=2568, title='Wikipedia: Jagdish Prasad Singh', abstract='Jagdish Prasad Singh is an Indian writer who writes in Hindi and English languages. The Government of India honoured him, in 2013, by awarding him Padma Shri, the fourth highest civilian award, for his contributions to the field of literature.', url='https://en.wikipedia.org/wiki/Jagdish_Prasad_Singh'), 7.397983437034528), (Abstract(ID=13002, title='Wikipedia: Vijay Prasad Dimri', abstract='| domesticpartner         =', url='https://en.wikipedia.org/wiki/Vijay_Prasad_Dimri'), 3.698991718517264), (Abstract(ID=1982, title='Wikipedia: Yaadhavam', abstract='Yaadhavam is a 1993 Indian Malayalam-language action film directed by Jomon and written by Ranjith, starring Suresh Gopi, Madhu, Narendra Prasad, and Khushbu..', url='https://en.wikipedia.org/wiki/Yaadhavam'), 3.698991718517264)]\n"
     ]
    }
   ],
   "source": [
    "# this will only download the xml dump if you don't have a copy already;\n",
    "# just delete the file if you want a fresh copy\n",
    "if not os.path.exists('data/enwiki-latest-abstract18.xml.gz'):\n",
    "    download_wikipedia_abstracts()\n",
    "    \n",
    "index = index_documents(load_documents(), Index())\n",
    "print(f'Index contains {len(index.documents)} documents')\n",
    "\n",
    "print('Intersection')\n",
    "print(index.search(\"Laxmi Prasad\", search_type='AND', rank=True))\n",
    "print()\n",
    "print('Union')\n",
    "print(index.search(\"Laxmi Prasad\", search_type='OR', rank=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04485dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3a965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66466b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
